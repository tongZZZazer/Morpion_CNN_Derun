# train_alpha_zero.py
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ai.alpha_model import AlphaZeroNet
from ai.self_play import play_self_game

# ğŸ”§ è¶…å‚æ•°
NUM_SELF_PLAY_GAMES = 300        # æ¯è½®è‡ªæˆ‘åšå¼ˆå±€æ•°
EPOCHS_PER_TRAINING = 20         # æ¯è½®è®­ç»ƒè½®æ•°
BATCH_SIZE = 32
REPLAY_BUFFER_SIZE = 20000       # æœ€å¤§ç»éªŒæ± å¤§å°
LR = 1e-4                         # å­¦ä¹ ç‡
TOTAL_ITERATIONS = 100           # è®­ç»ƒè¿­ä»£æ¬¡æ•°
SAVE_EVERY = 10                  # æ¯ N è½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹

def train(model, data, optimizer):
    model.train()
    losses = []

    for epoch in range(EPOCHS_PER_TRAINING):
        np.random.shuffle(data)
        for i in range(0, len(data), BATCH_SIZE):
            batch = data[i:i+BATCH_SIZE]

            states = torch.tensor([s for s, _, _ in batch], dtype=torch.float32).unsqueeze(1)
            pis = torch.tensor([p for _, p, _ in batch], dtype=torch.float32)
            zs = torch.tensor([z for _, _, z in batch], dtype=torch.float32).unsqueeze(1)

            optimizer.zero_grad()
            policy_logits, values = model(states)

            value_loss = nn.MSELoss()(values, zs)
            policy_loss = nn.KLDivLoss(reduction="batchmean")(
                nn.LogSoftmax(dim=1)(policy_logits), pis)

            loss = value_loss + policy_loss
            loss.backward()
            optimizer.step()
            losses.append(loss.item())

    return np.mean(losses)

def main():
    model = AlphaZeroNet()
    optimizer = optim.Adam(model.parameters(), lr=LR)
    replay_buffer = []

    for iteration in range(1, TOTAL_ITERATIONS + 1):
        print(f"\n=== Iteration {iteration} ===")

        # 1. è‡ªæˆ‘åšå¼ˆæ”¶é›†æ•°æ®
        all_data = []
        for _ in range(NUM_SELF_PLAY_GAMES):
            game_data = play_self_game(model, simulations=50)  # ğŸ” æ›´å¼ºæœç´¢
            all_data.extend(game_data)

        # 2. æ›´æ–°ç»éªŒæ± 
        replay_buffer.extend(all_data)
        if len(replay_buffer) > REPLAY_BUFFER_SIZE:
            replay_buffer = replay_buffer[-REPLAY_BUFFER_SIZE:]  # ä¿ç•™æœ€æ–°ç»éªŒ

        # 3. ç”¨ç»éªŒæ± è®­ç»ƒæ¨¡å‹
        avg_loss = train(model, replay_buffer, optimizer)
        print(f"å¹³å‡ Loss : {avg_loss:.4f}ï¼Œæ ·æœ¬æ•° : {len(replay_buffer)}")

        # 4. å®šæœŸä¿å­˜æ¨¡å‹
        if iteration % SAVE_EVERY == 0 or iteration == TOTAL_ITERATIONS:
            filename = f"checkpoint_alpha_iter{iteration}.pt"
            torch.save(model.state_dict(), filename)
            print(f"æ¨¡å‹å·²ä¿å­˜ï¼š{filename}")

if __name__ == "__main__":
    main()
